{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPm6SDBI57EzfvEw9EHcD2X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d239014a2ff84651b543a7436ffb925b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9cac5ed7f3ba4073ba15a93982f8096e",
              "IPY_MODEL_f0fae08a540746dfb40c16582d975cda",
              "IPY_MODEL_27624c1eb74b495183dcf76e16a8307c"
            ],
            "layout": "IPY_MODEL_dd0b95c69bc34762a8d8fb74fd1a7132"
          }
        },
        "9cac5ed7f3ba4073ba15a93982f8096e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b51dad5f7424ba9ba3812f87f0a3569",
            "placeholder": "​",
            "style": "IPY_MODEL_b975d4e4429e42d3a239a639c102eee1",
            "value": "Map: 100%"
          }
        },
        "f0fae08a540746dfb40c16582d975cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_012eb17b043e488fbdec06831e1940ff",
            "max": 600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a759c6b7532240b09e95f105dbac78eb",
            "value": 600
          }
        },
        "27624c1eb74b495183dcf76e16a8307c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d20094ed11b4a539c697bee8322ac9a",
            "placeholder": "​",
            "style": "IPY_MODEL_af6ff3a2367a4ed0a6e4250a387d4db2",
            "value": " 600/600 [00:00&lt;00:00, 4052.91 examples/s]"
          }
        },
        "dd0b95c69bc34762a8d8fb74fd1a7132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b51dad5f7424ba9ba3812f87f0a3569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b975d4e4429e42d3a239a639c102eee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "012eb17b043e488fbdec06831e1940ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a759c6b7532240b09e95f105dbac78eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d20094ed11b4a539c697bee8322ac9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af6ff3a2367a4ed0a6e4250a387d4db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ANISHOBJECT/Machine_Learning_Project/blob/main/Creative_Writing_Prompt_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# import random\n",
        "\n",
        "# def create_dataset(output_file=\"training_prompts.txt\"):\n",
        "#     \"\"\"Create a diverse dataset of writing prompts and save to a file.\"\"\"\n",
        "#     # Define genres, themes, and tones\n",
        "#     genres = ['fantasy', 'sci-fi', 'mystery', 'horror', 'romance', 'thriller', 'historical fiction', 'adventure', 'coming-of-age']\n",
        "#     themes = ['love', 'betrayal', 'redemption', 'identity', 'survival', 'hope', 'sacrifice', 'family']\n",
        "#     tones = ['mysterious', 'humorous', 'dark', 'uplifting', 'suspenseful', 'whimsical']\n",
        "\n",
        "#     # Base prompts\n",
        "#     base_prompts = [\n",
        "#         \"Write a {} story about a {} who discovers a {} secret.\",\n",
        "#         \"In a {} world, a {} must {} to save the {}.\", # This template requires 4 arguments\n",
        "#         \"Write a {} tale where {} leads to an unexpected {}.\",\n",
        "#         \"Create a {} narrative about a {} facing a {} challenge.\",\n",
        "#         \"Write about a {} who finds {} in a {} setting.\"\n",
        "#     ]\n",
        "\n",
        "#     # Generate prompts\n",
        "#     prompts = []\n",
        "#     for _ in range(100):  # Generate 100 prompts\n",
        "#         genre = random.choice(genres)\n",
        "#         theme = random.choice(themes)\n",
        "#         tone = random.choice(tones)\n",
        "#         prompt_template = random.choice(base_prompts)\n",
        "\n",
        "#         # Fill in the template with random elements\n",
        "#         # Check the number of replacement fields and provide the correct number of arguments\n",
        "#         if prompt_template.count('{}') == 4:\n",
        "#             # For the template with 4 fields, add an extra random element\n",
        "#             extra_element = random.choice(genres + themes + tones) # Combine lists for more variety\n",
        "#             prompt = prompt_template.format(genre, theme, tone, extra_element)\n",
        "#         else:\n",
        "#             # For templates with 3 fields, use the original arguments\n",
        "#             prompt = prompt_template.format(genre, theme, tone)\n",
        "\n",
        "#         prompts.append(prompt)\n",
        "\n",
        "#     # Save to file\n",
        "#     with open(output_file, 'w') as f:\n",
        "#         for prompt in prompts:\n",
        "#             f.write(prompt + '\\n')\n",
        "\n",
        "#     print(f\"Dataset created with {len(prompts)} prompts in {output_file}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     create_dataset()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuF0nYpHNOi4",
        "outputId": "c9d47a7b-7d1f-4a12-b183-a3df7b18b780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created with 100 prompts in training_prompts.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# import csv\n",
        "# import pandas as pd\n",
        "\n",
        "# def generate_creative_prompts(num_prompts=500):\n",
        "#     \"\"\"Generate creative writing prompts similar to the training data pattern\"\"\"\n",
        "\n",
        "#     # Define the components based on the training data patterns\n",
        "#     genres = [\n",
        "#         \"fantasy\", \"romance\", \"sci-fi\", \"mystery\", \"horror\", \"adventure\",\n",
        "#         \"thriller\", \"coming-of-age\", \"historical fiction\"\n",
        "#     ]\n",
        "\n",
        "#     themes_characters = [\n",
        "#         \"family\", \"sacrifice\", \"love\", \"hope\", \"identity\", \"betrayal\",\n",
        "#         \"survival\", \"redemption\"\n",
        "#     ]\n",
        "\n",
        "#     adjectives_moods = [\n",
        "#         \"mysterious\", \"humorous\", \"whimsical\", \"suspenseful\", \"uplifting\",\n",
        "#         \"dark\", \"unexpected\"\n",
        "#     ]\n",
        "\n",
        "#     story_types = [\"narrative\", \"story\", \"tale\"]\n",
        "\n",
        "#     actions = [\"facing\", \"discovers\", \"finds\", \"leads to\"]\n",
        "\n",
        "#     settings_contexts = [\n",
        "#         \"challenge\", \"secret\", \"setting\", \"world\"\n",
        "#     ]\n",
        "\n",
        "#     # Template patterns from the training data\n",
        "#     templates = [\n",
        "#         # Pattern 1: Create a [genre] [story_type] about a [theme] facing a [adjective] challenge.\n",
        "#         lambda: f\"Create a {random.choice(genres)} {random.choice(story_types)} about a {random.choice(themes_characters)} facing a {random.choice(adjectives_moods)} challenge.\",\n",
        "\n",
        "#         # Pattern 2: In a [genre] world, a [theme] must [adjective] to save the [theme].\n",
        "#         lambda: f\"In a {random.choice(genres)} world, a {random.choice(themes_characters)} must {random.choice(adjectives_moods)} to save the {random.choice(themes_characters)}.\",\n",
        "\n",
        "#         # Pattern 3: Write a [genre] [story_type] about a [theme] who discovers a [adjective] secret.\n",
        "#         lambda: f\"Write a {random.choice(genres)} {random.choice(story_types)} about a {random.choice(themes_characters)} who discovers a {random.choice(adjectives_moods)} secret.\",\n",
        "\n",
        "#         # Pattern 4: Write a [genre] tale where [theme] leads to an unexpected [adjective].\n",
        "#         lambda: f\"Write a {random.choice(genres)} tale where {random.choice(themes_characters)} leads to an unexpected {random.choice(adjectives_moods)}.\",\n",
        "\n",
        "#         # Pattern 5: Write about a [genre] who finds [theme] in a [adjective] setting.\n",
        "#         lambda: f\"Write about a {random.choice(genres)} who finds {random.choice(themes_characters)} in a {random.choice(adjectives_moods)} setting.\",\n",
        "\n",
        "#         # Additional creative patterns to add variety\n",
        "#         lambda: f\"Craft a {random.choice(genres)} epic where {random.choice(themes_characters)} becomes the key to {random.choice(adjectives_moods)} {random.choice(themes_characters)}.\",\n",
        "\n",
        "#         lambda: f\"Tell a {random.choice(genres)} legend about a {random.choice(themes_characters)} seeking {random.choice(adjectives_moods)} {random.choice(themes_characters)}.\",\n",
        "\n",
        "#         lambda: f\"Write a {random.choice(genres)} chronicle where {random.choice(themes_characters)} must overcome {random.choice(adjectives_moods)} obstacles.\",\n",
        "\n",
        "#         lambda: f\"Create a {random.choice(genres)} saga about the {random.choice(adjectives_moods)} journey of {random.choice(themes_characters)}.\",\n",
        "\n",
        "#         lambda: f\"In a {random.choice(adjectives_moods)} {random.choice(genres)} realm, {random.choice(themes_characters)} discovers the power of {random.choice(themes_characters)}.\",\n",
        "\n",
        "#         lambda: f\"Write a {random.choice(genres)} adventure where {random.choice(themes_characters)} transforms through {random.choice(adjectives_moods)} trials.\",\n",
        "\n",
        "#         lambda: f\"Create a {random.choice(genres)} odyssey focused on {random.choice(themes_characters)} and their {random.choice(adjectives_moods)} destiny.\",\n",
        "\n",
        "#         lambda: f\"Tell a {random.choice(genres)} story where {random.choice(themes_characters)} must choose between {random.choice(themes_characters)} and {random.choice(themes_characters)}.\",\n",
        "\n",
        "#         lambda: f\"Write about a {random.choice(genres)} world where {random.choice(themes_characters)} is both {random.choice(adjectives_moods)} and essential.\",\n",
        "\n",
        "#         lambda: f\"Craft a {random.choice(genres)} tale exploring how {random.choice(themes_characters)} leads to {random.choice(adjectives_moods)} {random.choice(themes_characters)}.\"\n",
        "#     ]\n",
        "\n",
        "#     prompts = []\n",
        "#     for i in range(num_prompts):\n",
        "#         # Select a random template and generate a prompt\n",
        "#         template = random.choice(templates)\n",
        "#         prompt = template()\n",
        "#         prompts.append(prompt)\n",
        "\n",
        "#     return prompts\n",
        "\n",
        "# def save_all_prompts_to_files(main_prompts, horror_prompts=None, romance_prompts=None, filename_base=\"Creative_Prompts_dataset_final\"):\n",
        "#     \"\"\"Save all generated prompts to both CSV and TXT files\"\"\"\n",
        "\n",
        "#     # Combine all prompts\n",
        "#     all_prompts = main_prompts.copy()\n",
        "\n",
        "#     # Add specialized prompts if provided\n",
        "#     if horror_prompts:\n",
        "#         all_prompts.extend(horror_prompts)\n",
        "#     if romance_prompts:\n",
        "#         all_prompts.extend(romance_prompts)\n",
        "\n",
        "#     # Save as TXT file\n",
        "#     txt_filename = f\"{filename_base}.txt\"\n",
        "#     with open(txt_filename, 'w', encoding='utf-8') as f:\n",
        "#         for i, prompt in enumerate(all_prompts, 1):\n",
        "#             f.write(f\"{prompt}\\n\")\n",
        "\n",
        "#     # Save as CSV file\n",
        "#     csv_filename = f\"{filename_base}.csv\"\n",
        "#     with open(csv_filename, 'w', newline='', encoding='utf-8') as f:\n",
        "#         writer = csv.writer(f)\n",
        "#         writer.writerow(['prompt_id', 'prompt_text', 'prompt_type'])  # Headers\n",
        "\n",
        "#         # Add main prompts\n",
        "#         for i, prompt in enumerate(main_prompts, 1):\n",
        "#             writer.writerow([i, prompt, 'general'])\n",
        "\n",
        "#         # Add horror prompts\n",
        "#         if horror_prompts:\n",
        "#             for i, prompt in enumerate(horror_prompts, len(main_prompts) + 1):\n",
        "#                 writer.writerow([i, prompt, 'horror_focused'])\n",
        "\n",
        "#         # Add romance prompts\n",
        "#         if romance_prompts:\n",
        "#             start_id = len(main_prompts) + len(horror_prompts) + 1 if horror_prompts else len(main_prompts) + 1\n",
        "#             for i, prompt in enumerate(romance_prompts, start_id):\n",
        "#                 writer.writerow([i, prompt, 'romance_focused'])\n",
        "\n",
        "#     print(f\"✅ All prompts saved successfully!\")\n",
        "#     print(f\"📄 TXT file: {txt_filename} ({len(all_prompts)} prompts)\")\n",
        "#     print(f\"📊 CSV file: {csv_filename} ({len(all_prompts)} prompts with metadata)\")\n",
        "\n",
        "#     return txt_filename, csv_filename\n",
        "\n",
        "# def create_detailed_csv_dataset(main_prompts, horror_prompts=None, romance_prompts=None, filename=\"Creative_Prompts_dataset_final_detailed.csv\"):\n",
        "#     \"\"\"Create a detailed CSV with additional metadata for each prompt\"\"\"\n",
        "\n",
        "#     all_data = []\n",
        "\n",
        "#     # Process main prompts\n",
        "#     for i, prompt in enumerate(main_prompts, 1):\n",
        "#         genre = extract_genre_from_prompt(prompt)\n",
        "#         theme = extract_theme_from_prompt(prompt)\n",
        "#         prompt_structure = classify_prompt_structure(prompt)\n",
        "\n",
        "#         all_data.append({\n",
        "#             'prompt_id': i,\n",
        "#             'prompt_text': prompt,\n",
        "#             'prompt_type': 'general',\n",
        "#             'primary_genre': genre,\n",
        "#             'main_theme': theme,\n",
        "#             'structure_type': prompt_structure,\n",
        "#             'word_count': len(prompt.split()),\n",
        "#             'character_count': len(prompt)\n",
        "#         })\n",
        "\n",
        "#     # Process horror prompts\n",
        "#     if horror_prompts:\n",
        "#         for i, prompt in enumerate(horror_prompts, len(main_prompts) + 1):\n",
        "#             theme = extract_theme_from_prompt(prompt)\n",
        "#             prompt_structure = classify_prompt_structure(prompt)\n",
        "\n",
        "#             all_data.append({\n",
        "#                 'prompt_id': i,\n",
        "#                 'prompt_text': prompt,\n",
        "#                 'prompt_type': 'horror_focused',\n",
        "#                 'primary_genre': 'horror',\n",
        "#                 'main_theme': theme,\n",
        "#                 'structure_type': prompt_structure,\n",
        "#                 'word_count': len(prompt.split()),\n",
        "#                 'character_count': len(prompt)\n",
        "#             })\n",
        "\n",
        "#     # Process romance prompts\n",
        "#     if romance_prompts:\n",
        "#         start_id = len(main_prompts) + len(horror_prompts) + 1 if horror_prompts else len(main_prompts) + 1\n",
        "#         for i, prompt in enumerate(romance_prompts, start_id):\n",
        "#             theme = extract_theme_from_prompt(prompt)\n",
        "#             prompt_structure = classify_prompt_structure(prompt)\n",
        "\n",
        "#             all_data.append({\n",
        "#                 'prompt_id': i,\n",
        "#                 'prompt_text': prompt,\n",
        "#                 'prompt_type': 'romance_focused',\n",
        "#                 'primary_genre': 'romance',\n",
        "#                 'main_theme': theme,\n",
        "#                 'structure_type': prompt_structure,\n",
        "#                 'word_count': len(prompt.split()),\n",
        "#                 'character_count': len(prompt)\n",
        "#             })\n",
        "\n",
        "#     # Create DataFrame and save\n",
        "#     df = pd.DataFrame(all_data)\n",
        "#     df.to_csv(filename, index=False, encoding='utf-8')\n",
        "\n",
        "#     print(f\"📊 Detailed CSV created: {filename}\")\n",
        "#     print(f\"📈 Dataset statistics:\")\n",
        "#     print(f\"   - Total prompts: {len(df)}\")\n",
        "#     print(f\"   - Prompt types: {df['prompt_type'].value_counts().to_dict()}\")\n",
        "#     print(f\"   - Average word count: {df['word_count'].mean():.1f}\")\n",
        "\n",
        "#     return filename\n",
        "\n",
        "# def extract_genre_from_prompt(prompt):\n",
        "#     \"\"\"Extract the primary genre from a prompt\"\"\"\n",
        "#     genres = [\"fantasy\", \"romance\", \"sci-fi\", \"mystery\", \"horror\", \"adventure\",\n",
        "#               \"thriller\", \"coming-of-age\", \"historical fiction\"]\n",
        "\n",
        "#     prompt_lower = prompt.lower()\n",
        "#     for genre in genres:\n",
        "#         if genre in prompt_lower:\n",
        "#             return genre\n",
        "#     return \"general\"\n",
        "\n",
        "# def extract_theme_from_prompt(prompt):\n",
        "#     \"\"\"Extract the main theme from a prompt\"\"\"\n",
        "#     themes = [\"family\", \"sacrifice\", \"love\", \"hope\", \"identity\", \"betrayal\",\n",
        "#               \"survival\", \"redemption\"]\n",
        "\n",
        "#     prompt_lower = prompt.lower()\n",
        "#     for theme in themes:\n",
        "#         if theme in prompt_lower:\n",
        "#             return theme\n",
        "#     return \"general\"\n",
        "\n",
        "# def classify_prompt_structure(prompt):\n",
        "#     \"\"\"Classify the structure type of the prompt\"\"\"\n",
        "#     prompt_lower = prompt.lower()\n",
        "\n",
        "#     if prompt_lower.startswith(\"create a\"):\n",
        "#         return \"create_narrative\"\n",
        "#     elif prompt_lower.startswith(\"write a\"):\n",
        "#         return \"write_story\"\n",
        "#     elif prompt_lower.startswith(\"in a\"):\n",
        "#         return \"world_setting\"\n",
        "#     elif prompt_lower.startswith(\"write about\"):\n",
        "#         return \"character_focus\"\n",
        "#     elif prompt_lower.startswith(\"tell a\"):\n",
        "#         return \"tell_story\"\n",
        "#     elif prompt_lower.startswith(\"craft a\"):\n",
        "#         return \"craft_narrative\"\n",
        "#     else:\n",
        "#         return \"other\"\n",
        "\n",
        "# def display_sample_prompts(prompts, num_samples=10):\n",
        "#     \"\"\"Display a sample of generated prompts\"\"\"\n",
        "#     print(f\"Sample of {num_samples} generated prompts:\")\n",
        "#     print(\"-\" * 50)\n",
        "#     for i, prompt in enumerate(random.sample(prompts, min(num_samples, len(prompts))), 1):\n",
        "#         print(f\"{i:2d}. {prompt}\")\n",
        "#     print(\"-\" * 50)\n",
        "\n",
        "# # Main execution\n",
        "# if __name__ == \"__main__\":\n",
        "#     print(\"🚀 Generating 500 creative writing prompts...\")\n",
        "\n",
        "#     # Generate the main prompts\n",
        "#     generated_prompts = generate_creative_prompts(500)\n",
        "\n",
        "#     # Display some examples\n",
        "#     display_sample_prompts(generated_prompts, 15)\n",
        "\n",
        "#     print(\"\\n🎯 Generating additional specialized sets...\")\n",
        "\n",
        "#     # Generate a horror-focused set\n",
        "#     horror_prompts = []\n",
        "#     for _ in range(50):\n",
        "#         templates_horror = [\n",
        "#             lambda: f\"Create a horror narrative about {random.choice(['family', 'sacrifice', 'betrayal', 'survival'])} facing a {random.choice(['mysterious', 'dark', 'suspenseful'])} challenge.\",\n",
        "#             lambda: f\"Write a horror tale where {random.choice(['betrayal', 'survival', 'sacrifice'])} leads to an unexpected {random.choice(['dark', 'mysterious'])}.\",\n",
        "#             lambda: f\"In a horror world, a {random.choice(['survival', 'betrayal', 'sacrifice'])} must {random.choice(['dark', 'mysterious', 'suspenseful'])} to save the {random.choice(['family', 'hope', 'identity'])}.\"\n",
        "#         ]\n",
        "#         horror_prompts.append(random.choice(templates_horror)())\n",
        "\n",
        "#     # Generate a romance-focused set\n",
        "#     romance_prompts = []\n",
        "#     for _ in range(50):\n",
        "#         templates_romance = [\n",
        "#             lambda: f\"Create a romance narrative about {random.choice(['love', 'sacrifice', 'hope'])} facing a {random.choice(['humorous', 'mysterious', 'uplifting'])} challenge.\",\n",
        "#             lambda: f\"Write a romance story about a {random.choice(['love', 'family', 'hope'])} who discovers a {random.choice(['mysterious', 'humorous', 'uplifting'])} secret.\",\n",
        "#             lambda: f\"In a romance world, a {random.choice(['love', 'sacrifice', 'identity'])} must {random.choice(['uplifting', 'mysterious', 'whimsical'])} to save the {random.choice(['hope', 'family', 'love'])}.\"\n",
        "#         ]\n",
        "#         romance_prompts.append(random.choice(templates_romance)())\n",
        "\n",
        "#     print(f\"✨ Generation complete!\")\n",
        "#     print(f\"📝 Main prompts: {len(generated_prompts)}\")\n",
        "#     print(f\"👻 Horror prompts: {len(horror_prompts)}\")\n",
        "#     print(f\"💕 Romance prompts: {len(romance_prompts)}\")\n",
        "#     print(f\"📊 Total prompts: {len(generated_prompts) + len(horror_prompts) + len(romance_prompts)}\")\n",
        "\n",
        "#     # Save all prompts to the final dataset files\n",
        "#     print(\"\\n💾 Saving to final dataset files...\")\n",
        "#     txt_file, csv_file = save_all_prompts_to_files(\n",
        "#         generated_prompts,\n",
        "#         horror_prompts,\n",
        "#         romance_prompts,\n",
        "#         \"Creative_Prompts_dataset_final\"\n",
        "#     )\n",
        "\n",
        "#     # Create detailed CSV with metadata\n",
        "#     print(\"\\n📊 Creating detailed dataset with metadata...\")\n",
        "#     detailed_csv = create_detailed_csv_dataset(\n",
        "#         generated_prompts,\n",
        "#         horror_prompts,\n",
        "#         romance_prompts,\n",
        "#         \"Creative_Prompts_dataset_final_detailed.csv\"\n",
        "#     )\n",
        "\n",
        "#     print(\"\\n🎉 All files created successfully!\")\n",
        "#     print(\"📁 Generated files:\")\n",
        "#     print(f\"   1. {txt_file}\")\n",
        "#     print(f\"   2. {csv_file}\")\n",
        "#     print(f\"   3. {detailed_csv}\")\n",
        "#     print(\"\\n✅ Dataset is ready for training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS-rdktYPMm8",
        "outputId": "450c27e2-64ee-4bbb-cb4e-701316d2f66b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Generating 500 creative writing prompts...\n",
            "Sample of 15 generated prompts:\n",
            "--------------------------------------------------\n",
            " 1. Write a thriller story about a betrayal who discovers a dark secret.\n",
            " 2. Create a horror saga about the unexpected journey of identity.\n",
            " 3. Create a fantasy odyssey focused on redemption and their dark destiny.\n",
            " 4. Write a thriller tale where love leads to an unexpected humorous.\n",
            " 5. Create a adventure tale about a hope facing a unexpected challenge.\n",
            " 6. Craft a romance tale exploring how sacrifice leads to suspenseful identity.\n",
            " 7. Create a thriller odyssey focused on love and their unexpected destiny.\n",
            " 8. Craft a romance epic where sacrifice becomes the key to humorous hope.\n",
            " 9. Tell a thriller story where sacrifice must choose between identity and hope.\n",
            "10. Write a romance tale about a family who discovers a dark secret.\n",
            "11. Create a fantasy saga about the whimsical journey of sacrifice.\n",
            "12. Tell a fantasy legend about a love seeking unexpected identity.\n",
            "13. Write a thriller tale where identity leads to an unexpected mysterious.\n",
            "14. Write a adventure chronicle where identity must overcome whimsical obstacles.\n",
            "15. Create a mystery odyssey focused on family and their mysterious destiny.\n",
            "--------------------------------------------------\n",
            "\n",
            "🎯 Generating additional specialized sets...\n",
            "✨ Generation complete!\n",
            "📝 Main prompts: 500\n",
            "👻 Horror prompts: 50\n",
            "💕 Romance prompts: 50\n",
            "📊 Total prompts: 600\n",
            "\n",
            "💾 Saving to final dataset files...\n",
            "✅ All prompts saved successfully!\n",
            "📄 TXT file: Creative_Prompts_dataset_final.txt (600 prompts)\n",
            "📊 CSV file: Creative_Prompts_dataset_final.csv (600 prompts with metadata)\n",
            "\n",
            "📊 Creating detailed dataset with metadata...\n",
            "📊 Detailed CSV created: Creative_Prompts_dataset_final_detailed.csv\n",
            "📈 Dataset statistics:\n",
            "   - Total prompts: 600\n",
            "   - Prompt types: {'general': 500, 'horror_focused': 50, 'romance_focused': 50}\n",
            "   - Average word count: 11.2\n",
            "\n",
            "🎉 All files created successfully!\n",
            "📁 Generated files:\n",
            "   1. Creative_Prompts_dataset_final.txt\n",
            "   2. Creative_Prompts_dataset_final.csv\n",
            "   3. Creative_Prompts_dataset_final_detailed.csv\n",
            "\n",
            "✅ Dataset is ready for training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# def prepare_dataset(csv_file=\"/content/Creative_Prompts_dataset_final.csv\", output_file=\"Ready_dataset_for_model.txt\"):\n",
        "#     \"\"\"Extract prompts from CSV and save to a text file.\"\"\"\n",
        "#     # Load the CSV file\n",
        "#     df = pd.read_csv(csv_file)\n",
        "\n",
        "#     # Extract the 'prompt_text' column\n",
        "#     prompts = df['prompt_text'].tolist()\n",
        "\n",
        "#     # Save to a text file, one prompt per line\n",
        "#     with open(output_file, 'w') as f:\n",
        "#         for prompt in prompts:\n",
        "#             f.write(prompt + '\\n')\n",
        "\n",
        "#     print(f\"Dataset prepared: {len(prompts)} prompts saved to {output_file}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     prepare_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESrUNlg6Qtxd",
        "outputId": "b4d18a86-be94-457e-b990-fb806f3ab537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset prepared: 600 prompts saved to Ready_dataset_for_model.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import os"
      ],
      "metadata": {
        "id": "gtj1OLdhS6F2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_model(training_file=\"/content/Ready_dataset_for_model (1).txt\", output_dir=\"./more_fine_tuned_model\"):\n",
        "    \"\"\"Fine-tune GPT-2 on the provided dataset.\"\"\"\n",
        "\n",
        "    # Disable wandb to avoid API key issues\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    print(\"Loading tokenizer and model...\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    # Prepare dataset\n",
        "    print(\"Preparing dataset...\")\n",
        "    with open(training_file, 'r', encoding='utf-8') as f:\n",
        "        prompts = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    print(f\"Loaded {len(prompts)} prompts from {training_file}\")\n"
      ],
      "metadata": {
        "id": "QrQLMuYgS8Ed"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def fine_tune_model(training_file=\"/content/Ready_dataset_for_model (1).txt\", output_dir=\"./updated_more_fine_tuned_model\"):\n",
        "    \"\"\"Fine-tune GPT-2 on the provided dataset.\"\"\"\n",
        "\n",
        "    # Disable wandb to avoid API key issues\n",
        "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "    # Load tokenizer and model\n",
        "    print(\"Loading tokenizer and model...\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    # Prepare dataset\n",
        "    print(\"Preparing dataset...\")\n",
        "    with open(training_file, 'r', encoding='utf-8') as f:\n",
        "        prompts = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    print(f\"Loaded {len(prompts)} prompts from {training_file}\")\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['text'], truncation=True, max_length=128, padding=\"max_length\")\n",
        "\n",
        "    dataset = Dataset.from_dict({'text': prompts})\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "    split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "    print(f\"Train dataset size: {len(split_dataset['train'])}\")\n",
        "    print(f\"Test dataset size: {len(split_dataset['test'])}\")\n",
        "\n",
        "    # Data collator\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    # Check device and set appropriate precision\n",
        "    device_type = str(model.device).split(':')[0]\n",
        "    use_fp16 = torch.cuda.is_available() and device_type == 'cuda'\n",
        "    use_bf16 = device_type == 'xla'  # For TPU\n",
        "\n",
        "    print(f\"Device: {model.device}\")\n",
        "    print(f\"Using fp16: {use_fp16}\")\n",
        "    print(f\"Using bf16: {use_bf16}\")\n",
        "\n",
        "    # Training arguments - FIXED VERSION\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=25,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        eval_strategy=\"steps\",  # CHANGED FROM evaluation_strategy\n",
        "        eval_steps=500,\n",
        "        logging_steps=100,\n",
        "        learning_rate=5e-5,\n",
        "        fp16=use_fp16,\n",
        "        bf16=use_bf16,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        report_to=[],  # Disable all reporting\n",
        "        save_safetensors=True,  # Use safetensors format\n",
        "        dataloader_pin_memory=False,  # Reduce memory usage\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    print(\"Initializing trainer...\")\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=split_dataset['train'],\n",
        "        eval_dataset=split_dataset['test'],\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    # Fine-tune\n",
        "    print(\"Starting fine-tuning...\")\n",
        "    print(\"This may take 15-30 minutes depending on your hardware...\")\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "        print(\"Training completed successfully!\")\n",
        "\n",
        "        # Save model\n",
        "        print(f\"Saving model to {output_dir}...\")\n",
        "        trainer.save_model(output_dir)\n",
        "        tokenizer.save_pretrained(output_dir)\n",
        "        print(f\"✅ Fine-tuned model saved to {output_dir}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error during training: {str(e)}\")\n",
        "        raise e\n",
        "\n"
      ],
      "metadata": {
        "id": "wr-M-II7RzDG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model_path=\"./updated_more_fine_tuned_model\", test_prompt=\"Create a fantasy story about\"):\n",
        "    \"\"\"Test the fine-tuned model with a sample prompt.\"\"\"\n",
        "    try:\n",
        "        print(f\"Loading fine-tuned model from {model_path}...\")\n",
        "        tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "        print(f\"Testing with prompt: '{test_prompt}'\")\n",
        "        inputs = tokenizer.encode(test_prompt, return_tensors='pt')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs,\n",
        "                max_length=100,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.8,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(\"Generated text:\")\n",
        "        print(\"-\" * 50)\n",
        "        print(generated_text)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error testing model: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Fine-tune the model\n",
        "    fine_tune_model()\n",
        "\n",
        "    # Test the fine-tuned model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TESTING THE FINE-TUNED MODEL\")\n",
        "    print(\"=\"*60)\n",
        "    test_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d239014a2ff84651b543a7436ffb925b",
            "9cac5ed7f3ba4073ba15a93982f8096e",
            "f0fae08a540746dfb40c16582d975cda",
            "27624c1eb74b495183dcf76e16a8307c",
            "dd0b95c69bc34762a8d8fb74fd1a7132",
            "7b51dad5f7424ba9ba3812f87f0a3569",
            "b975d4e4429e42d3a239a639c102eee1",
            "012eb17b043e488fbdec06831e1940ff",
            "a759c6b7532240b09e95f105dbac78eb",
            "9d20094ed11b4a539c697bee8322ac9a",
            "af6ff3a2367a4ed0a6e4250a387d4db2"
          ]
        },
        "id": "vQND-n3UQ8Uc",
        "outputId": "3e3c4c45-ae77-4bbd-859f-8dc13eb293fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model...\n",
            "Preparing dataset...\n",
            "Loaded 600 prompts from /content/Ready_dataset_for_model (1).txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d239014a2ff84651b543a7436ffb925b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 540\n",
            "Test dataset size: 60\n",
            "Device: cpu\n",
            "Using fp16: False\n",
            "Using bf16: False\n",
            "Initializing trainer...\n",
            "Starting fine-tuning...\n",
            "This may take 15-30 minutes depending on your hardware...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6750' max='6750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6750/6750 16:08, Epoch 25/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.793000</td>\n",
              "      <td>0.805470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.688300</td>\n",
              "      <td>0.722330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.637300</td>\n",
              "      <td>0.765963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.596200</td>\n",
              "      <td>0.792455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.580400</td>\n",
              "      <td>0.788737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.558200</td>\n",
              "      <td>0.804666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>0.818746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.515600</td>\n",
              "      <td>0.870685</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.476500</td>\n",
              "      <td>0.909089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.471400</td>\n",
              "      <td>0.957450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.425200</td>\n",
              "      <td>1.002808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.431000</td>\n",
              "      <td>1.026061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.416300</td>\n",
              "      <td>1.043767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed successfully!\n",
            "Saving model to ./updated_more_fine_tuned_model...\n",
            "✅ Fine-tuned model saved to ./updated_more_fine_tuned_model\n",
            "\n",
            "============================================================\n",
            "TESTING THE FINE-TUNED MODEL\n",
            "============================================================\n",
            "Loading fine-tuned model from ./updated_more_fine_tuned_model...\n",
            "Testing with prompt: 'Create a fantasy story about'\n",
            "Generated text:\n",
            "--------------------------------------------------\n",
            "Create a fantasy story about a sacrifice facing a suspenseful challenge.\n",
            " a love must suspenseful challenge. uplifting to save the suspenseful. dark hope. humorous. love discovers the power of hope.\n",
            "\n",
            "Saving the redemption through unexpected trials.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Create a zip file of your model folder\n",
        "shutil.make_archive('/content/fine_tuned_model', 'zip', '/content/fine_tuned_model')\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/fine_tuned_model.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "EsqCwGreXlmw",
        "outputId": "a72ecbd6-1afe-4460-8219-2aaff819c808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e3b3f6f3-5a31-422e-9872-10ce3aadbe01\", \"fine_tuned_model.zip\", 1842076930)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy the model folder to your Drive\n",
        "shutil.copytree('/content/fine_tuned_model', '/content/drive/MyDrive/fine_tuned_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ShNAG8DZrlBZ",
        "outputId": "47ec55d6-f7af-40ba-c092-f16ca1ecfd9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/fine_tuned_model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_path = \"/content/more_fine_tuned_model\"  # or \"path/to/checkpoint-135\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "JGtyMh__39HP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7f6XFm34Uwy",
        "outputId": "411177d8-220c-49d8-aaea-5885ac853f83"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, how are you? a mystery tale where betrayal leads to an unexpected mysterious.\n",
            "\n",
            "In a unexpected setting, betrayal discovers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import random"
      ],
      "metadata": {
        "id": "TwQv8bHz5VqB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import random\n",
        "\n",
        "\n",
        "class PromptGenerator:\n",
        "    def __init__(self, model_path=\"/content/more_fine_tuned_model\"):\n",
        "        \"\"\"\n",
        "        Initialize the Prompt Generator using a pretrained or fine-tuned GPT-2 model.\n",
        "        :param model_path: Path to the fine-tuned model or 'gpt2' for the base model.\n",
        "        \"\"\"\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_path).to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def build_prompt(self, genre=\"\", theme=\"\", tone=\"\"):\n",
        "        \"\"\"\n",
        "        Construct the base prompt string based on user input.\n",
        "        \"\"\"\n",
        "        prompt = \"Write a detailed\"\n",
        "        if genre:\n",
        "            prompt += f\" {genre}\"\n",
        "        prompt += \" story\"\n",
        "        if theme:\n",
        "            prompt += f\" about {theme}\"\n",
        "        if tone:\n",
        "            prompt += f\" with a {tone} tone\"\n",
        "        prompt += \".\"\n",
        "        return prompt\n",
        "\n",
        "    def generate_prompt(self, genre=\"\", theme=\"\", tone=\"\"):\n",
        "        \"\"\"\n",
        "        Generate a continuation for the writing prompt using the model.\n",
        "        \"\"\"\n",
        "        base_prompt = self.build_prompt(genre, theme, tone)\n",
        "        input_ids = self.tokenizer.encode(base_prompt, return_tensors='pt').to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = self.model.generate(\n",
        "                input_ids,\n",
        "                max_length=input_ids.shape[1] + 100,\n",
        "                temperature=0.8,\n",
        "                top_k=50,\n",
        "                top_p=0.95,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                repetition_penalty=1.1,\n",
        "            )\n",
        "\n",
        "        generated_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        result = generated_text[len(base_prompt):].strip()\n",
        "        if not result.endswith(('.', '!', '?')):\n",
        "            result += '.'\n",
        "        return base_prompt + \" \" + result.capitalize()\n",
        "\n",
        "    def interactive_session(self):\n",
        "        \"\"\"\n",
        "        Run an interactive console session for generating writing prompts.\n",
        "        \"\"\"\n",
        "        print(\"=== Creative Writing Prompt Generator ===\")\n",
        "        while True:\n",
        "            print(\"\\nOptions:\\n 1. Custom Prompt\\n 2. Random Prompt\\n 3. Exit\")\n",
        "            choice = input(\"Choose an option (1-3): \").strip()\n",
        "\n",
        "            if choice == \"1\":\n",
        "                genre = input(\"Enter Genre (e.g., fantasy): \").strip()\n",
        "                theme = input(\"Enter Theme (e.g., redemption): \").strip()\n",
        "                tone = input(\"Enter Tone (e.g., mysterious): \").strip()\n",
        "                prompt = self.generate_prompt(genre, theme, tone)\n",
        "                print(f\"\\nGenerated Prompt:\\n{prompt}\\n\")\n",
        "\n",
        "            elif choice == \"2\":\n",
        "                genres = ['fantasy', 'sci-fi', 'mystery', 'horror', 'romance']\n",
        "                themes = ['love', 'betrayal', 'redemption', 'identity', 'survival']\n",
        "                tones = ['mysterious', 'humorous', 'dark', 'uplifting', 'suspenseful']\n",
        "                genre = random.choice(genres)\n",
        "                theme = random.choice(themes)\n",
        "                tone = random.choice(tones)\n",
        "                prompt = self.generate_prompt(genre, theme, tone)\n",
        "                print(f\"\\nRandom Prompt:\\n{prompt}\\n\")\n",
        "\n",
        "            elif choice == \"3\":\n",
        "                print(\"Happy writing! Goodbye.\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                print(\"Invalid choice. Please enter 1, 2, or 3.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace \"gpt2\" with the path to your fine-tuned model if needed\n",
        "    generator = PromptGenerator(model_path=\"/content/more_fine_tuned_model\")\n",
        "    generator.interactive_session()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNVxRpNJ5TUs",
        "outputId": "cdbc8f62-4fa3-4b06-be97-a27ba9784559"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Creative Writing Prompt Generator ===\n",
            "\n",
            "Options:\n",
            " 1. Custom Prompt\n",
            " 2. Random Prompt\n",
            " 3. Exit\n",
            "Choose an option (1-3): 2\n",
            "\n",
            "Random Prompt:\n",
            "Write a detailed fantasy story about love with a dark tone. Within the coming-of age world, family discovers how hope leads to humorous survival.\n",
            " of identity. inspired by historical fiction and their suspenseful destiny.\n",
            "\n",
            "\n",
            "Options:\n",
            " 1. Custom Prompt\n",
            " 2. Random Prompt\n",
            " 3. Exit\n",
            "Choose an option (1-3): 1\n",
            "Enter Genre (e.g., fantasy): fantasy\n",
            "Enter Theme (e.g., redemption): love\n",
            "Enter Tone (e.g., mysterious): dark\n",
            "\n",
            "Generated Prompt:\n",
            "Write a detailed fantasy story about love with a dark tone. Written by an unexpected who discovers the power of betrayal. the sacrifice must suspenseful to save the family.\n",
            "- mystery identity!\n",
            "\n",
            "\n",
            "Options:\n",
            " 1. Custom Prompt\n",
            " 2. Random Prompt\n",
            " 3. Exit\n",
            "Choose an option (1-3): 3\n",
            "Happy writing! Goodbye.\n"
          ]
        }
      ]
    }
  ]
}